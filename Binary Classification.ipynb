{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./ColonTumor/colonTumor.data\",delimiter=',' ,header=None,)  #Importing the Data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1          2          3          4          5     \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "        6          7          8          9       ...         1991       1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762    ...     67.56125  259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524    ...     92.23875   96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821    ...     82.71500   31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071    ...     41.68375    5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131    ...     76.60375  161.35000   \n",
       "\n",
       "        1993      1994        1995       1996       1997      1998      1999  \\\n",
       "0  138.89875  88.23250   39.667857   67.82875   75.67750  83.52250  28.70125   \n",
       "1  150.59000  82.23750   85.033333  152.19500  186.56750  44.47250  16.77375   \n",
       "2  193.92000  76.97250  224.620240   31.22500   42.65625  16.09250  15.15625   \n",
       "3  183.00625  74.52875   67.710714   48.33875   42.52000  49.98250  16.08500   \n",
       "4   61.70125  54.56375  223.359520   73.09875   57.59875   7.48875  31.81250   \n",
       "\n",
       "       2000  \n",
       "0  negative  \n",
       "1  positive  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  negative  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x == \"negative\":                # Assigning 0 if the class label is negative\n",
    "        return 0\n",
    "    else:                              # Assigning 1 if the class label is positive \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[2000] = map(lambda x: convert(x), df[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1          2          3          4          5     \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "        6          7          8          9     ...       1991       1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762  ...   67.56125  259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524  ...   92.23875   96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821  ...   82.71500   31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071  ...   41.68375    5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131  ...   76.60375  161.35000   \n",
       "\n",
       "        1993      1994        1995       1996       1997      1998      1999  \\\n",
       "0  138.89875  88.23250   39.667857   67.82875   75.67750  83.52250  28.70125   \n",
       "1  150.59000  82.23750   85.033333  152.19500  186.56750  44.47250  16.77375   \n",
       "2  193.92000  76.97250  224.620240   31.22500   42.65625  16.09250  15.15625   \n",
       "3  183.00625  74.52875   67.710714   48.33875   42.52000  49.98250  16.08500   \n",
       "4   61.70125  54.56375  223.359520   73.09875   57.59875   7.48875  31.81250   \n",
       "\n",
       "   2000  \n",
       "0     0  \n",
       "1     1  \n",
       "2     0  \n",
       "3     1  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    40\n",
      "1    22\n",
      "Name: 2000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[2000].value_counts()      # We have only 22 positive examples out of the total 62 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's assign the features and labels to different objects.\n",
    "y = np.array(df[2000])\n",
    "X = np.array(df.drop(2000, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's split the data into train and test set, we will keep only 10% for testing since the amount of data is small. \n",
    "# Later we will use cross validation on the training set to get better estimates of error. \n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the features to a lower dimension with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d1a3e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "components = [i for i in range(40)]\n",
    "explained_variance = []\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components=component)\n",
    "    pca.fit(X_train_normalized)\n",
    "    explained_variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.plot(components, explained_variance, marker='o')\n",
    "plt.axhline(0.99, color='black')\n",
    "plt.axvline(38, color='black')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looks like with 38 principal components we are retaining 99% of variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=38)\n",
    "X_train_normalized_pca = pca.fit_transform(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_normalized_pca = pca.transform(X_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i) Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "skf = StratifiedKFold(y_train, n_folds=5)   # Running k-fold cross validation with 5 folds \n",
    "best_lr_model = None                        # Objects which will keep the best model after all the iterations\n",
    "max_cv_accuracy = 0.0                       # Max cv accuracy reported by the classifier\n",
    "choices_regularization_param = [0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24] # Choices for lambda \n",
    "dict_performance = {}                       # Dictionary to keep f1 score for each value of lambda \n",
    "\n",
    "for lambda_ in choices_regularization_param:\n",
    "    current_f1_score = 0.0 \n",
    "    for train_index, test_index in skf:     # Using l2 penalty for regularization for preventing overfitting. \n",
    "        clf = LogisticRegression(C=lambda_, penalty='l2', solver='liblinear', n_jobs=2, random_state=0)\n",
    "        clf.fit(X_train_normalized_pca[train_index], y_train[train_index])\n",
    "        y_pred = clf.predict(X_train_normalized_pca[test_index])\n",
    "        current_f1_score += f1_score(y_train[test_index], y_pred)\n",
    "    \n",
    "    dict_performance[lambda_] = current_f1_score/5.0\n",
    "    if current_f1_score/5.0 > max_cv_accuracy:\n",
    "        max_cv_accuracy = current_f1_score/5.0\n",
    "        best_lr_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.01: 0.76323232323232326,\n",
       " 0.02: 0.70666666666666667,\n",
       " 0.04: 0.63676767676767687,\n",
       " 0.08: 0.63676767676767687,\n",
       " 0.16: 0.63676767676767687,\n",
       " 0.32: 0.64767676767676785,\n",
       " 0.64: 0.64767676767676785,\n",
       " 1.28: 0.60767676767676782,\n",
       " 2.56: 0.60767676767676782,\n",
       " 5.12: 0.60767676767676782,\n",
       " 10.24: 0.60767676767676782}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test = best_lr_model.predict(X_test_normalized_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print f1_score(y_test, pred_test)        # F1 score on test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ii) Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
