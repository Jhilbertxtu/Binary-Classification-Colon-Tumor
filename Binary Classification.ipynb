{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./ColonTumor/colonTumor.data\",delimiter=',' ,header=None,)  #Importing the Data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1          2          3          4          5     \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "        6          7          8          9       ...         1991       1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762    ...     67.56125  259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524    ...     92.23875   96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821    ...     82.71500   31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071    ...     41.68375    5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131    ...     76.60375  161.35000   \n",
       "\n",
       "        1993      1994        1995       1996       1997      1998      1999  \\\n",
       "0  138.89875  88.23250   39.667857   67.82875   75.67750  83.52250  28.70125   \n",
       "1  150.59000  82.23750   85.033333  152.19500  186.56750  44.47250  16.77375   \n",
       "2  193.92000  76.97250  224.620240   31.22500   42.65625  16.09250  15.15625   \n",
       "3  183.00625  74.52875   67.710714   48.33875   42.52000  49.98250  16.08500   \n",
       "4   61.70125  54.56375  223.359520   73.09875   57.59875   7.48875  31.81250   \n",
       "\n",
       "       2000  \n",
       "0  negative  \n",
       "1  positive  \n",
       "2  negative  \n",
       "3  positive  \n",
       "4  negative  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x == \"negative\":                # Assigning 0 if the class label is negative\n",
    "        return 0\n",
    "    else:                              # Assigning 1 if the class label is positive \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[2000] = map(lambda x: convert(x), df[2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8589.4163</td>\n",
       "      <td>5468.2409</td>\n",
       "      <td>4263.4075</td>\n",
       "      <td>4064.9357</td>\n",
       "      <td>1997.8929</td>\n",
       "      <td>5282.3250</td>\n",
       "      <td>2169.7200</td>\n",
       "      <td>2773.4212</td>\n",
       "      <td>7526.3862</td>\n",
       "      <td>4607.6762</td>\n",
       "      <td>...</td>\n",
       "      <td>67.56125</td>\n",
       "      <td>259.91250</td>\n",
       "      <td>138.89875</td>\n",
       "      <td>88.23250</td>\n",
       "      <td>39.667857</td>\n",
       "      <td>67.82875</td>\n",
       "      <td>75.67750</td>\n",
       "      <td>83.52250</td>\n",
       "      <td>28.70125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9164.2537</td>\n",
       "      <td>6719.5295</td>\n",
       "      <td>4883.4487</td>\n",
       "      <td>3718.1589</td>\n",
       "      <td>2015.2214</td>\n",
       "      <td>5569.9071</td>\n",
       "      <td>3849.0588</td>\n",
       "      <td>2793.3875</td>\n",
       "      <td>7017.7338</td>\n",
       "      <td>4802.2524</td>\n",
       "      <td>...</td>\n",
       "      <td>92.23875</td>\n",
       "      <td>96.27625</td>\n",
       "      <td>150.59000</td>\n",
       "      <td>82.23750</td>\n",
       "      <td>85.033333</td>\n",
       "      <td>152.19500</td>\n",
       "      <td>186.56750</td>\n",
       "      <td>44.47250</td>\n",
       "      <td>16.77375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3825.7050</td>\n",
       "      <td>6970.3614</td>\n",
       "      <td>5369.9688</td>\n",
       "      <td>4705.6500</td>\n",
       "      <td>1166.5536</td>\n",
       "      <td>1572.1679</td>\n",
       "      <td>1325.4025</td>\n",
       "      <td>1472.2587</td>\n",
       "      <td>3296.9512</td>\n",
       "      <td>2786.5821</td>\n",
       "      <td>...</td>\n",
       "      <td>82.71500</td>\n",
       "      <td>31.10250</td>\n",
       "      <td>193.92000</td>\n",
       "      <td>76.97250</td>\n",
       "      <td>224.620240</td>\n",
       "      <td>31.22500</td>\n",
       "      <td>42.65625</td>\n",
       "      <td>16.09250</td>\n",
       "      <td>15.15625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6246.4487</td>\n",
       "      <td>7823.5341</td>\n",
       "      <td>5955.8350</td>\n",
       "      <td>3975.5643</td>\n",
       "      <td>2002.6131</td>\n",
       "      <td>2130.5429</td>\n",
       "      <td>1531.1425</td>\n",
       "      <td>1714.6312</td>\n",
       "      <td>3869.7850</td>\n",
       "      <td>4989.4071</td>\n",
       "      <td>...</td>\n",
       "      <td>41.68375</td>\n",
       "      <td>5.92500</td>\n",
       "      <td>183.00625</td>\n",
       "      <td>74.52875</td>\n",
       "      <td>67.710714</td>\n",
       "      <td>48.33875</td>\n",
       "      <td>42.52000</td>\n",
       "      <td>49.98250</td>\n",
       "      <td>16.08500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3230.3287</td>\n",
       "      <td>3694.4500</td>\n",
       "      <td>3400.7400</td>\n",
       "      <td>3463.5857</td>\n",
       "      <td>2181.4202</td>\n",
       "      <td>2922.7821</td>\n",
       "      <td>2069.2463</td>\n",
       "      <td>2948.5750</td>\n",
       "      <td>3303.3712</td>\n",
       "      <td>3109.4131</td>\n",
       "      <td>...</td>\n",
       "      <td>76.60375</td>\n",
       "      <td>161.35000</td>\n",
       "      <td>61.70125</td>\n",
       "      <td>54.56375</td>\n",
       "      <td>223.359520</td>\n",
       "      <td>73.09875</td>\n",
       "      <td>57.59875</td>\n",
       "      <td>7.48875</td>\n",
       "      <td>31.81250</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1          2          3          4          5     \\\n",
       "0  8589.4163  5468.2409  4263.4075  4064.9357  1997.8929  5282.3250   \n",
       "1  9164.2537  6719.5295  4883.4487  3718.1589  2015.2214  5569.9071   \n",
       "2  3825.7050  6970.3614  5369.9688  4705.6500  1166.5536  1572.1679   \n",
       "3  6246.4487  7823.5341  5955.8350  3975.5643  2002.6131  2130.5429   \n",
       "4  3230.3287  3694.4500  3400.7400  3463.5857  2181.4202  2922.7821   \n",
       "\n",
       "        6          7          8          9     ...       1991       1992  \\\n",
       "0  2169.7200  2773.4212  7526.3862  4607.6762  ...   67.56125  259.91250   \n",
       "1  3849.0588  2793.3875  7017.7338  4802.2524  ...   92.23875   96.27625   \n",
       "2  1325.4025  1472.2587  3296.9512  2786.5821  ...   82.71500   31.10250   \n",
       "3  1531.1425  1714.6312  3869.7850  4989.4071  ...   41.68375    5.92500   \n",
       "4  2069.2463  2948.5750  3303.3712  3109.4131  ...   76.60375  161.35000   \n",
       "\n",
       "        1993      1994        1995       1996       1997      1998      1999  \\\n",
       "0  138.89875  88.23250   39.667857   67.82875   75.67750  83.52250  28.70125   \n",
       "1  150.59000  82.23750   85.033333  152.19500  186.56750  44.47250  16.77375   \n",
       "2  193.92000  76.97250  224.620240   31.22500   42.65625  16.09250  15.15625   \n",
       "3  183.00625  74.52875   67.710714   48.33875   42.52000  49.98250  16.08500   \n",
       "4   61.70125  54.56375  223.359520   73.09875   57.59875   7.48875  31.81250   \n",
       "\n",
       "   2000  \n",
       "0     0  \n",
       "1     1  \n",
       "2     0  \n",
       "3     1  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    40\n",
      "1    22\n",
      "Name: 2000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df[2000].value_counts()      # We have only 22 positive examples out of the total 62 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's assign the features and labels to different objects.\n",
    "y = np.array(df[2000])\n",
    "X = np.array(df.drop(2000, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's split the data into train and test set, we will keep only 10% for testing since the amount of data is small. \n",
    "# Later we will use cross validation on the training set to get better estimates of error. \n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming the features to a lower dimension with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEPCAYAAABGP2P1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1166c6e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "components = [i for i in range(50)]\n",
    "explained_variance = []\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components=component)\n",
    "    pca.fit(X_train_normalized)\n",
    "    explained_variance.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.plot(components, explained_variance, marker='o')\n",
    "plt.axhline(0.99, color='black')\n",
    "plt.axvline(38, color='black')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, looks like with 38 principal components we are retaining 97.92% of variance in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=38)\n",
    "X_train_normalized_pca = pca.fit_transform(X_train_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.979216702958\n"
     ]
    }
   ],
   "source": [
    "print np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_normalized_pca = pca.transform(X_test_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i) Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "skf = StratifiedKFold(y_train, n_folds=5)   # Running k-fold cross validation with 5 folds \n",
    "best_lr_model = None                        # Objects which will keep the best model after all the iterations\n",
    "max_cv_accuracy = 0.0                       # Max cv accuracy reported by the classifier\n",
    "choices_regularization_param = [0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24] # Choices for lambda \n",
    "dict_performance_lr = {}                       # Dictionary to keep auc score for each value of lambda \n",
    "\n",
    "for lambda_ in choices_regularization_param:\n",
    "    current_auc_score = []\n",
    "    for train_index, test_index in skf:     # Using l2 penalty for regularization for preventing overfitting. \n",
    "        clf = LogisticRegression(C=lambda_, penalty='l2', n_jobs=2, random_state=0)\n",
    "        clf.fit(X_train_normalized_pca[train_index], y_train[train_index])\n",
    "        y_pred = clf.predict(X_train_normalized_pca[test_index])\n",
    "        current_auc_score.append(roc_auc_score(y_train[test_index], y_pred))\n",
    "    \n",
    "    dict_performance_lr[lambda_] = [np.mean(current_auc_score), np.std(current_auc_score)]\n",
    "    if np.mean(current_auc_score) > max_cv_accuracy:\n",
    "        max_cv_accuracy = np.mean(current_auc_score)\n",
    "        best_lr_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.090633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.081127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.109731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.710714</td>\n",
       "      <td>0.109731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.119523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.119523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.119523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.28</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.148805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.56</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.148805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.12</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.148805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.24</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.148805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lambda      Mean       Std\n",
       "2     0.01  0.817857  0.090633\n",
       "3     0.02  0.760714  0.081127\n",
       "0     0.32  0.710714  0.109731\n",
       "5     0.64  0.710714  0.109731\n",
       "1     0.16  0.696429  0.119523\n",
       "4     0.04  0.696429  0.119523\n",
       "10    0.08  0.696429  0.119523\n",
       "6     1.28  0.671429  0.148805\n",
       "7     2.56  0.671429  0.148805\n",
       "8     5.12  0.671429  0.148805\n",
       "9    10.24  0.671429  0.148805"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_performance = pd.DataFrame([[x[0], x[1][0], x[1][1]] for x in dict_performance_lr.items()], \\\n",
    "                             columns=[\"lambda\", \"Mean\", \"Std\"])\n",
    "lr_performance.sort_values(by=['Mean'], ascending=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=2,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test = best_lr_model.predict(X_test_normalized_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print roc_auc_score(y_test, pred_test)        # AUC score on test data \n",
    "print accuracy_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ii) Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "skf = StratifiedKFold(y_train, n_folds=5)                            # Running k-fold cross validation with 5 folds \n",
    "C_params = [0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24] # Choices of the regularization param \n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']                       # Choices of kernels. \n",
    "best_svm_model = None                                                # Best Model after training \n",
    "max_cv_accuracy = 0.0                                                # Max average cross validation score. \n",
    "dict_performance_svm = {}\n",
    "\n",
    "for c in C_params:\n",
    "    for kernel_ in kernels:\n",
    "        current_auc_score = []\n",
    "        for train_index, test_index in skf:     \n",
    "            clf = SVC(C=c, kernel=kernel_, random_state=0)\n",
    "            clf.fit(X_train_normalized_pca[train_index], y_train[train_index])\n",
    "            y_pred = clf.predict(X_train_normalized_pca[test_index])\n",
    "            current_auc_score.append(roc_auc_score(y_train[test_index], y_pred))\n",
    "            \n",
    "        dict_performance_svm[(c, kernel_)] = [np.mean(current_auc_score), np.std(current_auc_score)]\n",
    "        if np.mean(current_auc_score) > max_cv_accuracy:\n",
    "            max_cv_accuracy = np.mean(current_auc_score)\n",
    "            best_svm_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Kernel</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.163663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.02</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.124540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.32</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.64</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.04</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.28</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.16</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.56</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.24</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.08</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.12</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.096890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.04</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.32</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.08</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.16</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.24</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.02</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.56</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.12</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.28</td>\n",
       "      <td>poly</td>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.104369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.28</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.56</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.02</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.16</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.64</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.32</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.24</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.16</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.08</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.04</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.04</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.28</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.12</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.24</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.12</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.08</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.02</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.64</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.01</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.56</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.32</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C   Kernel      Mean       Std\n",
       "1    0.01   linear  0.767857  0.163663\n",
       "8    0.02   linear  0.757143  0.124540\n",
       "22   0.32   linear  0.742857  0.096890\n",
       "17   0.64   linear  0.742857  0.096890\n",
       "39   0.04   linear  0.742857  0.096890\n",
       "3    1.28   linear  0.742857  0.096890\n",
       "38   0.16   linear  0.742857  0.096890\n",
       "5    2.56   linear  0.742857  0.096890\n",
       "32  10.24   linear  0.742857  0.096890\n",
       "26   0.08   linear  0.742857  0.096890\n",
       "11   5.12   linear  0.742857  0.096890\n",
       "35   0.04     poly  0.717857  0.104369\n",
       "28   0.01     poly  0.717857  0.104369\n",
       "24   0.32     poly  0.717857  0.104369\n",
       "23   0.08     poly  0.717857  0.104369\n",
       "19   0.16     poly  0.717857  0.104369\n",
       "0   10.24     poly  0.717857  0.104369\n",
       "15   0.02     poly  0.717857  0.104369\n",
       "14   2.56     poly  0.717857  0.104369\n",
       "13   5.12     poly  0.717857  0.104369\n",
       "2    0.64     poly  0.717857  0.104369\n",
       "4    1.28     poly  0.717857  0.104369\n",
       "16   1.28  sigmoid  0.500000  0.000000\n",
       "7    2.56  sigmoid  0.500000  0.000000\n",
       "42   0.02  sigmoid  0.500000  0.000000\n",
       "41   0.16      rbf  0.500000  0.000000\n",
       "40   0.64      rbf  0.500000  0.000000\n",
       "37   0.32  sigmoid  0.500000  0.000000\n",
       "36  10.24  sigmoid  0.500000  0.000000\n",
       "6    0.16  sigmoid  0.500000  0.000000\n",
       "34   0.08  sigmoid  0.500000  0.000000\n",
       "33   0.04  sigmoid  0.500000  0.000000\n",
       "30   0.04      rbf  0.500000  0.000000\n",
       "31   1.28      rbf  0.500000  0.000000\n",
       "18   5.12      rbf  0.500000  0.000000\n",
       "29  10.24      rbf  0.500000  0.000000\n",
       "9    5.12  sigmoid  0.500000  0.000000\n",
       "27   0.08      rbf  0.500000  0.000000\n",
       "10   0.02      rbf  0.500000  0.000000\n",
       "25   0.64  sigmoid  0.500000  0.000000\n",
       "12   0.01  sigmoid  0.500000  0.000000\n",
       "21   2.56      rbf  0.500000  0.000000\n",
       "20   0.32      rbf  0.500000  0.000000\n",
       "43   0.01      rbf  0.500000  0.000000"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_performance = pd.DataFrame([[x[0][0], x[0][1], x[1][0], x[1][1]] for x in dict_performance_svm.items()], \\\n",
    "                             columns=[\"C\", \"Kernel\", \"Mean\", \"Std\"])\n",
    "svm_performance.sort_values(by=['Mean'], ascending=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.01, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test_svm = best_svm_model.predict(X_test_normalized_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.857142857143\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(y_test, pred_test_svm)        # Auc score on Test set\n",
    "print accuracy_score(y_test, pred_test_svm)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble methods - Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "skf = StratifiedKFold(y_train, n_folds=5)   # Running k-fold cross validation with 5 folds \n",
    "best_rf_model = None                        # Objects which will keep the best model after all the iterations\n",
    "max_cv_accuracy = 0.0                       # Max cv accuracy reported by the classifier\n",
    "number_of_estimators = [5, 10, 20, 30, 50, 100, 200] # Choices for number of trees  \n",
    "dict_performance_rf = {}                       # Dictionary to keep AUC score for each value of n_estimator\n",
    "\n",
    "for n_tree_ in number_of_estimators:\n",
    "    current_auc_score = [] \n",
    "    for train_index, test_index in skf:     # Using l2 penalty for regularization for preventing overfitting. \n",
    "        clf = RandomForestClassifier(n_estimators=n_tree_, random_state=0)\n",
    "        clf.fit(X_train_normalized_pca[train_index], y_train[train_index])\n",
    "        y_pred = clf.predict(X_train_normalized_pca[test_index])\n",
    "        current_auc_score.append(roc_auc_score(y_train[test_index], y_pred))\n",
    "    \n",
    "    dict_performance_rf[n_tree_] = [np.mean(current_auc_score), np.std(current_auc_score)]\n",
    "    if np.mean(current_auc_score) > max_cv_accuracy:\n",
    "        max_cv_accuracy = np.mean(current_auc_score)\n",
    "        best_rf_model = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of trees</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.102519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.083299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.653571</td>\n",
       "      <td>0.091473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.101393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>0.621429</td>\n",
       "      <td>0.088497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30</td>\n",
       "      <td>0.617857</td>\n",
       "      <td>0.065465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.084061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of trees      Mean       Std\n",
       "1                5  0.675000  0.102519\n",
       "5               20  0.657143  0.083299\n",
       "3               10  0.653571  0.091473\n",
       "2              200  0.635714  0.101393\n",
       "4               50  0.621429  0.088497\n",
       "6               30  0.617857  0.065465\n",
       "0              100  0.610714  0.084061"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_performance = pd.DataFrame([[x[0], x[1][0], x[1][1]] for x in dict_performance_rf.items()], \\\n",
    "                             columns=[\"Number of trees\", \"Mean\", \"Std\"])\n",
    "rf_performance.sort_values(by=['Mean'], ascending=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test_rf = best_rf_model.predict(X_test_normalized_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.857142857143\n"
     ]
    }
   ],
   "source": [
    "print roc_auc_score(y_test, pred_test_rf)        # AUC score on test data \n",
    "print accuracy_score(y_test, pred_test_rf)       # Accuracy of the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
